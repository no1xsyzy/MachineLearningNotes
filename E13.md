##EM算法

如果定义
$$
J\left( Q,\theta \right) = \sum_{i}\sum_{z^{\left( i \right)}}Q_i\left( z^{\left( i \right)} \right)\log\frac{p\left( x^{\left( i \right)},z^{\left( i \right)};\theta \right)}{Q_i\left( z^{\left( i \right)} \right)}
$$
则EM算法其实是$J$上的坐标上升，其中：

* **E-期望化步骤**实为对$Q$做坐标上升
* **M-最大化步骤**实为对$\theta$做坐标上升

##重新讨论混合高斯模型
**E-期望化步骤**比较简单，计算：
$$w_j^{\left ( i \right )} = Q_i\left ( z^{\left ( i \right )}=j \right )=P\left ( z^{\left ( i \right )}=j|x^{\left ( i \right )};\phi,\mu,\Sigma \right )$$
**M-最大化步骤**需要通过调节参数达到最大化$J$。
因为$x^{\left ( i \right )}|z^{\left ( i \right )}=j;\mu,\Sigma \sim \mathcal{N}\left( \mu_j,\Sigma_j \right )$且$p\left ( z^{\left ( i \right )}=j;\phi \right ) = \phi_j$，
$$
\begin{aligned}
J\left( Q,\theta \right) &= \sum_{i=1}^{m}\sum_{z^{\left( i \right)}}Q_i\left( z^{\left( i \right)} \right)\log\frac{p\left( x^{\left( i \right)},z^{\left( i \right)};\phi,\mu,\Sigma \right)}{Q_i\left( z^{\left( i \right)} \right)}\\
&=\sum_{i=1}^{m}\sum_{j=1}^{k}Q_i\left( z^{\left( i \right)}=j \right)\log\frac{p\left ( x^{\left ( i \right )}|z^{\left ( i \right )}=j;\mu,\Sigma \right )p\left ( z^{\left ( i \right )}=j;\phi \right )}{Q_i\left( z^{\left( i \right)}=j \right)}\\
&=\sum_{i=1}^{m}\sum_{j=1}^{k}w_j^{\left ( i \right )}\log\frac{\frac{1}{\left ( 2\pi \right )^{n/2}\left | \Sigma_j \right |^{1/2}}\exp\left ( -\frac{1}{2}\left ( x^{\left ( i \right )}-\mu_j \right )^T\Sigma_j^{-1}\left ( x^{\left ( i \right )}-\mu_j \right ) \right )\cdot\phi_j}{w_j^{\left ( i \right )}}
\end{aligned}
$$
对$\mu_l$求梯度得到
$$
\nabla_{\mu_l}J\left( Q,\theta \right)=\sum_{i=1}^{m}w_l^{\left ( i \right )}\left ( \Sigma_l^{-1}x^{\left ( i \right )}-\Sigma_l^{-1}\mu_l \right )\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily set}}}{=}}0
$$
解得
$$\mu_l := \frac{\sum_{i=1}^{m}w_l^{\left ( i \right )}x^{\left ( i \right )}}{\sum_{i=1}^{m}w_l^{\left ( i \right )}}$$
同理可解$\Sigma$，对于$\phi$要注意保持$\sum_{j}\phi_j=1$。最终可得
$$\phi_j:=\frac{1}{m}\sum_{i=1}^{m}w_j^{\left( i \right)}$$

##例子-文档分集群
类似之前的垃圾邮件分类，特征建立为$\left\{ x^{\left( 1 \right)},\dots,x^{\left( m \right)} \right\}$，其中$x^{\left( i \right)} \in \left \{ 0,1 \right \}^{n}$，$x_j^{\left( i \right)}$表示在第$\left( i \right)$份文档中是否存在第$j$个单词。

* **E-期望化步骤**：
$$w^{\left ( i \right )}=P\left ( z^{\left ( i \right )}=1|x^{\left ( i \right )};\phi_{j|z},\phi \right )$$
* **M-最大化步骤**：
$$
\begin{aligned}
\phi_{j|z=1} &:= \frac{\sum_{i=1}^{m}w^{\left ( i \right )}1\left \{ x_j^{\left ( i \right )}=1 \right \}}{\sum_{i=1}^{m}w^{\left ( i \right )}}\\
\phi_{j|z=0} &:= \frac{\sum_{i=1}^{m}\left ( 1-w^{\left ( i \right )} \right )1\left \{ x_j^{\left ( i \right )}=1 \right \}}{\sum_{i=1}^{m}\left ( 1-w^{\left ( i \right )} \right )}\\
\phi_z&:=\frac{\sum_{i=1}^{m}w^{\left ( i \right )}}{m}
\end{aligned}s
$$

#因子分析
如果待分类的点数不能远大于特征维度，容易导致协方差矩阵不可逆。为了解决这个问题，可对协方差做一些限制。

##对$\Sigma$的限制
$\Sigma$是个对角线矩阵，$$\Sigma_{jj}=\frac{1}{m}\sum_{i=1}^{m}\left( x_j^{\left( i \right)}-\mu_j \right)^2$$
即，$\Sigma_{jj}$是第$j$个维度的数据的方差。

可以更进一步地限制协方差矩阵，使得对角线上的元素全部相等，即$\Sigma=\sigma^2I$，其中$\sigma^2$是可控的，其最大似然估计为$$\sigma^2 = \frac{1}{mn}\sum_{j=1}^{n}\sum_{i=1}^{m}\left( x_j^{\left( i \right)}-\mu_j \right)^2$$

##高斯分布的边缘分布和条件分布
假设此种表示法：
$$
x = \begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
$$
其中$x_1 \in \mathbb{R}^r$，$x_2 \in \mathbb{R}^s$，$x \in \mathbb{R}^{r+s}$。假定$x\sim\mathcal{N}\left ( \mu,\Sigma \right )$，其中：
$$
\mu = \begin{bmatrix}
\mu_1\\
\mu_2
\end{bmatrix}
\quad
\Sigma = \begin{bmatrix}
\Sigma_{11}&\Sigma_{12}\\
\Sigma_{21}&\Sigma_{22}
\end{bmatrix}
$$
其中$\mu_1 \in \mathbb{R}^r$，$\mu_2 \in \mathbb{R}^s$，$\Sigma_{11} \in \mathbb{R}^{r\times r}$，$\Sigma_{12} \in \mathbb{R}^{r\times s}$。注意协方差矩阵为对称矩阵，$\Sigma_{12} = \Sigma_{21}^T$。

<!-- ###因子分析模型
$$
\begin{aligned}
z&\sim\mathcal{N}\left( 0,I \right)\\
x|z&\sim\mathcal{N}\left( \mu+\Lambda z,\Psi \right)
\end{aligned}
$$ -->
