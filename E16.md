#增强学习（Reinforced Learning）
##马可夫决策过程（MDP）
输入为：

* $S$ - **状态**集合
* $A$ - **行动**集合
* $P_{sa}$ - 状态转移分布
    $$\sum_{s'}P_{sa}\left ( s' \right )=1 , P_{sa}\left ( s' \right )\geq0$$
* $\gamma$ - **贴现因子**
    $$0\leq\gamma<1$$
* $R$ - **反馈函数**
    $$R:S\mapsto\mathbb{R}$$

过程建模为：
$$s_0\overset{a_0}{\rightarrow}s_1\overset{a_1}{\rightarrow}s_2\overset{a_2}{\rightarrow}s_3\overset{a_3}{\rightarrow}\dots$$
其中，$s_{t+1}\sim P_{s_t a_t}$

输出为

* $\pi$ - 策略
    $$\pi:S\mapsto A$$
    $$\pi = \arg\max_{\pi}E\left [ R\left ( s_0 \right )+\gamma R\left ( s_1 \right )+\gamma^2 R\left ( s_2 \right )+\dots \right ]$$

##决策过程
对于任意$\pi$，定义价值函数$V^\pi:S\mapsto\mathbb{R}$使得$V^\pi\left( s \right)$是以$s$为初始状态、执行策略$\pi$时反馈总量的数学期望：
$$V^\pi\left ( s \right ) = E\left[ R\left ( s_0 \right )+\gamma R\left ( s_1 \right )+\gamma^2 R\left ( s_2 \right )+\dots \middle| s_0=s,\pi \right]$$
给定$\pi$时，可以看到$V^\pi$是动态规划问题：
$$V^\pi\left( s \right) = R\left( s \right)+\gamma\sum_{s'\in S}P_{s\pi\left( s \right)}\left( s' \right)V^\pi\left( s' \right)$$

定义优化函数$V^*:S\mapsto\mathbb{R}$，并整理：
$$
\begin{aligned}
V^*\left ( s \right ) &= \max_\pi V^\pi\left ( s \right )\\
&=R\left ( s \right )+\max_{a\in A}\gamma\sum_{s'\in S}P_{sa}\left ( s' \right )V^*\left ( s' \right )
\end{aligned}
$$

定义最优策略函数$\pi^*:S\mapsto A$：
$$\pi^*\left ( s \right ) = \arg\max_{a\in A}\sum_{s'\in S}P_{sa}\left ( s' \right )V^*\left ( s' \right )$$

##价值迭代

1.  初始化$\forall s,V\left ( s \right ) = 0$
2.  重复直到收敛：
    对每个$s$，更新：
    $$V\left ( s \right ) := R\left ( s \right )+\max_{a\in A}\gamma\sum_{s'\in S}P_{sa}\left ( s' \right )V\left ( s' \right )$$

重复过程，$V\left( s \right)\to V^*\left( s \right)$。

此处的更新有两种方式：

1. **同步**更新，整体替换，$V := B\left( V \right)$
2. **异步**更新，逐个处理替换，所以同次循环中早前更新的数值用作最新的数值。

最终，$V$会收敛于$V^*$，再使用$\pi^*\left ( s \right ) = \arg\max_{a\in A}\sum_{s'\in S}P_{sa}\left ( s' \right )V^*\left ( s' \right )$可以得到$\pi^*$

##策略迭代

1.  随机初始化$\pi$
2.  重复直到收敛：
    a.  使得$V := V^\pi$
    b.  对于每个$s$，使得
        $$\pi\left ( s \right ) := \arg\max_{a\in A}\sum_{s'\in S}P_{sa}\left ( s' \right )V\left ( s' \right )$$

最终，$V$会收敛于$V^*$，且$\pi$会收敛于$\pi^*$

##$P_{sa}$不确定的情况
每次执行策略$\pi$后通过执行数据重新学习$P_{sa}$

