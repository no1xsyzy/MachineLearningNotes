#学习理论
##$\mathcal{H}$为无穷集的情况
###引入VC维度
给定集合$\mathcal{S}=\left\{ x^{\left( 1 \right)},\dots,x^{\left( d \right)} \right\}$为一点集（与训练集无关）$x^{^{\left( i \right)}}\in\mathcal{X}$，若$\mathcal{H}$能够实现对$\mathcal{S}$的任意划分，称$\mathcal{H}$**打散**了$\mathcal{S}$。

而对于给定的假设集$\mathcal{H}$，其VC维$\mathrm{VC} \left( \mathcal{H} \right)$是最大的能打散的点集的大小。

###定理
给定$\mathcal{H}$，使得$d=\mathrm{VC} \left ( \mathcal{H} \right )$，在至少$1-\delta$的可能性下，对于所有$h\in \mathcal{H}$，
$$\left| \varepsilon\left( h \right)-\hat{\varepsilon}\left( h \right) \right| \leq O \left( \sqrt{ \frac{d}{m} \log\frac{m}{d} + \frac{1}{m} \log\frac{1}{\delta} } \right)$$
因此也有，在至少$1-\delta$的可能性下，
$$\varepsilon\left( \hat{h} \right) \leq \varepsilon\left( h^* \right)+O \left( \sqrt{ \frac{d}{m} \log\frac{m}{d} + \frac{1}{m} \log\frac{1}{\delta} } \right)$$

###结论
练习样本的数量和训练参数的数量成正比。

#模型选择
有一组有限多的模型，$\mathcal{M}=\left\{ M_1,\dots,M_d \right\}$
要选取一个最好的模型

##交叉验证
假设这种方法：

1. 对每个模型训练出一个假设
2. 选择训练误差最小的一个

显然是不行的，因为过拟合的训练误差最小。

###简单交叉验证

1. 随机选取70%的数据作为训练数据$\mathcal{S}_{\textrm{train}}$，剩下的作为交叉验证数据集$\mathcal{S}_{\textrm{CV}}$。
2. 使用每个模型$M_i$，仅对$\mathcal{S}_{\textrm{train}}$进行训练，得到假设$h_i$。
3. 对每个假设$h_i$计算在交叉验证数据集$\mathcal{S}_{\textrm{CV}}$上的经验误差$\hat{\varepsilon}_{\mathcal{S}_{\textrm{CV}}}\left( h_i \right)$。选择经验误差最小的。

###切分交叉验证

1. 将$\mathcal{S}$随机切分为$k$个互不重叠的训练样本$\mathcal{S}_1,\dots,\mathcal{S}_k$
2. 对每个模型$M_i$和每个训练样本切片$\mathcal{S}_j$，训练整个样本除了这个切片之外的所有切片的并集，得到假设$h_{ij}$，计算在$\mathcal{S}$上的经验误差$\hat{\varepsilon}_{\mathcal{S}_j\left( h_{ij} \right)$。
3. 选择经验误差平均值最小的模型$M_i$，然后对整个训练样本训练一个假设作为最终假设。

通常$k=10$，其他数值也可以。特别地，$k=m$也就是每次只排除一个，称为**排一交叉验证**。

##特征选取
###正向搜索
初始特征集$\mathcal{F}=\varnothing$，不断尝试增加一个特征，增加特征的过程会扫描整个特征集，选择交叉验证的结果最好的。最终选择最好的特征子集。

###反向搜索
与正向搜索相反，初始特征集$\mathcal{F}$为特征全集。而每次尝试删除一个特征。

###筛选特征选择
测试每个特征的信息量。（应该是类似香农的信息论的观点）
