#主成分分析-PCA
##算法步骤

1. 数据标准化为零均值和单位方差
2. 计算$\Sigma=\frac{1}{m}\sum_{i=1}^{m}x^{\left( i \right)}{x^{\left( i \right)}}^T$
3. 找到$\Sigma$的前$k$个主特征向量

非常巨大的维度的数据（比如人脸识别，100x100维的图像灰度）导致$\Sigma$维度更加巨大（10000x10000维）

##隐含语义索引
通过之前的“第n个单词是否存在”的建模法，一篇文档的内容可能具有50000维。

相似性函数：
$$
\begin{aligned}
\textup{sim}\left( x^{\left( i \right)},x^{\left( j \right)} \right) &= \cos\left( \theta \right)\\ 
 &= \frac{{x^{\left( i \right)}}^T x^{\left( j \right)}}{\left\| x^{\left( i \right)} \right\|\left\| x^{\left( j \right)} \right\|}
\end{aligned}
$$
对于study和learn两个单词，被认为是不用的单词
但是通过主成分分析，计算的是投影到子空间的结果，两个单词就能更具有相关性。

##奇异值分解（SVD）
$A_{m\times n} = U_{m\times n}D_{n\times n}V^T_{n\times n}$
$U$的列为$AA^T$的特征向量
$V$的列为$A^TA$的特征向量
只需要用Matlab或者Octave的`svd`命令。

#独立成分分析（ICA）
##基本信息
有数据$s \in \mathbb{R}^n$是由n个独立来源生成的。而能观测的内容为$x=As$。其中$A$被称为混合矩阵。不断地观察可以得到一个数据集$\left\{ x^{\left( i \right)};i=1,\dots,m \right\}$而我们的目标是从$x^{\left( i \right)}$中还原$s^{\left( i \right)}$。

在声音分离的例子中，$s^{\left( i \right)}$是一个$n$维向量，$s^{\left( i \right)}_j$表示的是第$j$个说话者在$i$时刻发出的声音，而$x^{\left( i \right)}_j$表示的是第$j$个话筒在$i$时刻收到的声音。

$W=A^{-1}$为还原矩阵，目标是找到$W$，用于还原$s^{\left( i \right)}=Wx^{\left( i \right)}$。为了方便，将$W$的第$i$行记作$w_i^T$。

##歧义

* 假定一个排列矩阵$P$，每一行和每一列均恰有一个$1$，那么$W$和$PW$是无法区分的。
* $w_i$的大小无法区分，如果$A$变为$2A$而$s^{\left( i \right)}$变为$0.5s^{\left( i \right)}$，那么结果$x^{\left( i \right)}$也是一样的。

但是，对于声音分离的例子，这些歧义并没有影响：顺序并不是关注点，$w_i$的大小也只会影响音量。

##密度和线性变换
$$p_x\left( x \right) = p_s\left( Wx \right)\left| W \right|$$
其中，$W = A^{-1}$

##ICA算法
$$
W := W+\alpha\left(
\begin{bmatrix}
1-2g\left( w_1^Tx^{\left( i \right)} \right)\\ 
1-2g\left( w_2^Tx^{\left( i \right)} \right)\\ 
\vdots \\ 
1-2g\left( w_n^Tx^{\left( i \right)} \right)
\end{bmatrix}
{x^{\left( i \right)}}^T+\left( W^T \right)^{-1} \right)
$$

