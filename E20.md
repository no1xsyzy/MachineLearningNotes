#部分可观察马尔可夫决策过程（POMDP）
问题为七元组：$\left( S,A,Y,\left\{ P_{sa} \right\},\left\{ O_s \right\},T,R \right)$

其中：

* $Y$为可能的观察结果集
* $O_s$为观察分布
    在任何一个步骤，观测结果$y_t \sim O_{s_t}$（如果处于状态$s_t$）

##策略搜索

> 定义一个策略集合$\Pi$，搜索一个好的策略$\pi \in \Pi$。

类比于监督学习：

> 定义一个假设集合$\mathcal{H}$，搜索一个好的假设$h \in \mathcal{H}$。

重新定义：

> 一随机策略为一函数$\pi : S\times A\mapsto\mathbb{R}$  
> $\pi\left( s,a \right)$为在$s$状态下选择动作$a$的可能性。  
> $\sum_a\pi\left( s,a \right) = 1$，$\pi\left( s,a \right) \geq 0$

1. 循环：
    1. 采样$s_0,a_0,s_1,a_1,\dots,s_T,a_T$；
    2. 计算$\textup{payoff} = R\left( s_0,a_0 \right)+\dots+R\left( s_T,a_T \right)$；
    3. 更新$\theta := \theta+\alpha\left[ \frac{\nabla_\theta\pi_\theta\left( s_0,a_0 \right)}{\pi_\theta\left( s_0,a_0 \right)}+\dots+\frac{\nabla_\theta\pi_\theta\left( s_T,a_T \right)}{\pi_\theta\left( s_T,a_T \right)} \right]\times\textup{payoff}$

##

$\hat{s}$为$s$的近似（$\hat{s}$为卡尔曼滤波中的$s_{t|t}$）
$$\pi_\theta\left( \hat{s},a \right) = \frac{1}{1+e^{-\theta^T\hat{s}}}$$

##Pegasus
在模拟中，如果使用随机，会导致模拟结果产生少许不同，影响增强学习算法效果。

反而采用每次不同的随机，则可以使得模拟结果非常接近期望。

