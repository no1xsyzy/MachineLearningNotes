#增强学习算法调试
##举例
算法不能很好地控制直升机的情况下可能的情况：

1. 如果算法能够很好地控制模拟器中的直升机，但现实中的飞机却不能很好地控制，
    那么是模拟器的问题；否则：
2. 如果$V^{\pi_\textup{RL}}\left( s_0 \right) < V^{\pi_\textup{human}}\left( s_0 \right)$（算法控制评价劣于人类控制评价），
    那么是算法不够好；否则：
3. 如果$V^{\pi_\textup{RL}}\left( s_0 \right) \geq V^{\pi_\textup{human}}\left( s_0 \right)$（算法控制评价优于人类控制评价），
    那么是评估函数不够好。

仅作例子，实际情况可能不同。

#微分动态规划（DDP）
在每次学习得到一个新的行动模式后重新通过模拟器采集控制轨迹数据并线性化。
尽管$s_{t+1}=f\left( s_t,a_t \right)$并没有改变，但$\bar{s_t}, \bar{a_t}$不同导致了线性化后的内容有所不同。

#卡尔曼滤波（Kalman filter）
实际中，观测结果可能与真实状态有偏差（噪音），卡尔曼滤波正是用于处理这个问题的方法。

设真实状态为$s_t$，观测结果为$y_t$，希望得到$P\left( s_t \middle| y_1,\dots,y_t \right)$。

$s_0,s_1,\dots,s_t,y_1,\dots,y_t$具有一联合高斯分布。
$$
z = \begin{bmatrix}
s_0\\ 
s_1\\ 
\vdots\\
s_t\\
y_1\\
\vdots\\
y_t
\end{bmatrix}, z \sim \mathcal{N}\left( \mu , \Sigma \right)
$$
这种方法虽然可以得到结果，但计算量十分巨大。

##过程

1. 预测步骤：
$$s_t|y_1,\dots,y_t \sim \mathcal{N}\left( s_{t|t},\Sigma_{t|t} \right)$$
然后：
$$s_{t+1}|y_1,\dots,y_t \sim \mathcal{N}\left( s_{t+1|t},\Sigma_{t+1|t} \right)$$
其中：
$$
\begin{aligned}
s_{t+1|t} &= As_{t|t}\\
\Sigma_{t+1|t} &= A\Sigma_{t|t}A^T+\Sigma_v
\end{aligned}
$$

2. 更新步骤
$$s_{t+1}|y_1,\dots,y_{t+1} \sim \mathcal{N}\left( s_{t+1|t+1},\Sigma_{t+1|t+1} \right)$$
其中
$$
\begin{aligned}
s_{t+1|t+1} &= s_{t+1|t}+K_{t+1}\left( y_{t+1}-Cs_{t+1|t} \right)\\
K_{t+1} &= \Sigma_{t+1|t}C^T\left( C\Sigma_{t+1|t}C^T+\Sigma_v \right)^{-1}\\
\Sigma_{t+1|t+1} &= \Sigma_{t+1|t}-\Sigma_{t+1|t}C^T\left( C\Sigma_{t+1|t}C^T+\Sigma_v \right)^{-1}C\Sigma_{t+1|t}
\end{aligned}
$$
这里$s_{t+1|t+1}$就是对$s_{t+1}$的最佳估计。

##回到有动作的情况
$$
\begin{aligned}
s_{t+1} &= As_t+Ba_t+w_t &&\bigl( w_t\sim\mathcal{N}\left( 0,\Sigma_w \right) \bigr)\\
y_t &= Cs_t+v_t &&\bigl( v_t\sim\mathcal{N}\left( 0,\Sigma_v \right) \bigr)\\
\end{aligned}
$$

具体步骤：

1. $s_{0|0}=s_0,\Sigma_{0|0}=0\bigl( \text{for }s_0\sim\mathcal{N}\left( s_{0|0},\Sigma_{0|0} \right) \bigr)$
2. 预测：
$$
\begin{aligned}
S_{t+1|t} &= As{t|t}+Ba_t\\
\Sigma_{t+1|t} &= A\Sigma_{t|t}A^T+\Sigma_v\\
a_t &= L_t\cdot s_{t|t} 
\end{aligned}
$$

