顺序最小优化算法
====

核函数（Kernel）
----

为了能够表示复杂的特征，采用核函数将两个原始特征向量以一个复杂度较低的函数映射到高维空间内的内积。

$$
\begin{aligned}
&K\left(x^{\left(i\right)},x^{\left(j\right)}\right)=\left \langle \phi\left(x^{\left(i\right)}\right),\phi\left(x^{\left(j\right)}\right) \right \rangle\\
&K : \mathbb{R}^n \times \mathbb{R}^n \mapsto \mathbb{R}
\end{aligned}
$$
一个例子：
$$
\begin{aligned}
K\left ( x,z \right )&=\left ( x^T z \right )^2\\
&=\left ( \sum_{i=1}^{n}x_i z_i \right )\left ( \sum_{j=1}^{n}x_j z_j \right )\\
&=\sum_{i=1}^{n}\sum_{j=1}^{n}\left ( x_i x_j \right )\left ( z_i z_j \right )\\
\phi\left ( x \right )&=\begin{bmatrix}
x_1x_1\\
x_1x_2\\
x_1x_3\\
x_2x_1\\
x_2x_2\\
x_2x_3\\
x_3x_1\\
x_3x_2\\
x_3x_3
\end{bmatrix}
\end{aligned}
$$

将3维的特征以$\phi$的方式映射到9维，并且运算开支非常小。甚至一些核函数可以将特征映射到无穷维空间，这是通过常规方式无法实现的：计算机不能处理无穷的数据。

核函数是否是可用的（valid）可以通过这样判断：
对于$K : \mathbb{R}^n \times \mathbb{R}^n \mapsto \mathbb{R}$，

软间隔（soft margin）
----

有的时候，通过设定核函数并不能轻易地将数据组分开，比如有一个/些异常数据，那么尝试将这个数据正确分类反而可能造成更多数据预测分类不正确。

所以，引入软间隔，使得一些点可以不正确分类，同时对这样的数加入惩罚量。问题变为：
$$
\begin{aligned}
\min_{\gamma,w,b}\quad &\frac{1}{2}\left \| w \right \|^2+C\sum_{i=1}^{n}\xi_1\\
\text{s.t.}\quad &y^{\left( i \right )}\left(w^T x^{\left(i \right )} +b \right ) \geq 1-\xi_i,\quad i=1,\dots ,m\\
&\xi_i \geq 0,\quad i=1,\dots,m
\end{aligned}
$$
同样是凸优化问题。

SMO算法
----

###坐标下降法
在凸优化问题上，可以每次找一个变量的最优，不断轮换找最优的变量能够得到最终的全局最优。

###SMO算法
因为$a_i$之间具有关联性，如果$a_2,\dots,a_m$固定，那么$a_1$也必须固定，所以转而采用两个一起改变的策略。每次改变两个变量，找到其中的最优解。
