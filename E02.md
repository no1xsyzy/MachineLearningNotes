E02-监督学习应用.梯度下降
====

2017-06-25

本期的内容主要是梯度下降（gradient descent）算法的原理。

符号
----
$m$：训练样本大小
$x$：输入变量/特征
$y$：输出变量/目标变量
$\left(x^{\left(i\right)} , y^{\left(i\right)}\right)$：第(i)组训练样本
$\left(x,y\right)$：训练样本
$h$：假设，即学习后的结果
$\theta_{\left(n+1\right)\times 1}$：训练参数

批量梯度下降法（batch gradient descent/BGD）
----
实质就是爬山算法：初始设定theta为0，通过向方差更低的位置不断移动，能够找出局部最优点。
移动的过程通过计算偏微分来向较低处移动。

随机梯度下降法（stochastic gradient descent/SGD）
----
每次仅对一个样本学习，最终也可以收敛于最优解。

正规方程（normal equations）
----
通过很长一段推导，最终将目标<方差最小化>转化为一个方程$$X^{T}X\theta=X^{T}y$$
